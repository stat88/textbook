
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5.4. Unbiased Estimators &#8212; Data 88S Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/Chapter_05/04_Unbiased_Estimators';</script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="5.5. Conditional Expectation" href="05_Conditional_Expectation.html" />
    <link rel="prev" title="5.3. Method of Indicators" href="03_Method_of_Indicators.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/data88s_logo.png" class="logo__image only-light" alt="Data 88S Textbook - Home"/>
    <script>document.write(`<img src="../../_static/data88s_logo.png" class="logo__image only-dark" alt="Data 88S Textbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="http://stat88.org">Course Home</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Chapters</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_01/00_The_Basics.html">1. The Basics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/01_Probabilities_as_Proportions.html">1.1. Probabilities as Proportions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/02_Exact_Calculation_or_Bound.html">1.2. Exact Calculation, or Bound?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/03_Fundamental_Rules.html">1.3. Fundamental Rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_01/04_Exercises.html">1.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_02/00_Intersections_and_Conditioning.html">2. Intersections and Conditioning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/01_The_Chance_of_an_Intersection.html">2.1. The Chance of an Intersection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/02_Symmetry_in_Simple_Random_Sampling.html">2.2. Symmetry in Simple Random Sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/03_Bayes_Rule.html">2.3. Bayes’ Rule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/04_Use_and_Interpretation.html">2.4. Use and Interpretation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/05_Independence.html">2.5. Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_02/06_Exercises.html">2.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_03/00_Random_Counts.html">3. Random Counts</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/01_Success_and_Failure.html">3.1. Success and Failure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/02_Random_Variables.html">3.2. Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/03_The_Binomial_Distribution.html">3.3. The Binomial Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/04_The_Hypergeometric_Distribution.html">3.4. The Hypergeometric Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/05_Examples.html">3.5. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_03/06_Exercises.html">3.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_04/00_Infinitely_Many_Values.html">4. Infinitely Many Values</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/01_Cumulative_Distribution_Function.html">4.1. Cumulative Distribution Function (CDF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/02_Waiting_Times.html">4.2. Waiting Times</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/03_Exponential_Approximations.html">4.3. Exponential Approximations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/04_The_Poisson_Distribution.html">4.4. The Poisson Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_04/05_Exercises.html">4.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="00_Expectation.html">5. Expectation</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_Definition.html">5.1. Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="02_Functions_of_Random_Variables.html">5.2. Functions of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_Method_of_Indicators.html">5.3. Method of Indicators</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">5.4. Unbiased Estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_Conditional_Expectation.html">5.5. Conditional Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="06_Expectation_by_Conditioning.html">5.6. Expectation by Conditioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="07_Exercises.html">5.7. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_06/00_Measuring_Variability.html">6. Measuring Variability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/01_Variance_and_Standard_Deviation.html">6.1. Variance and Standard Deviation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/02_Simplifying_the_Calculation.html">6.2. Simplifying the Calculation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/03_Markovs_Inequality.html">6.3. Markov’s Inequality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/04_Chebyshevs_Inequality.html">6.4. Chebyshev’s Inequality</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_06/05_Exercises.html">6.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_07/00_The_Variance_of_a_Sum.html">7. The Variance of a Sum</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/01_Sums_of_Independent_Random_Variables.html">7.1. Sums of Independent Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/02_Sampling_Without_Replacement.html">7.2. Sampling Without Replacement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/03_The_Law_of_Averages.html">7.3. The Law of Averages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_07/04_Exercises.html">7.4. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_08/00_Central_Limit_Theorem.html">8. Central Limit Theorem</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/01_Distribution_of_a_Sample_Sum.html">8.1. The Distribution of a Sample Sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/02_Standard_Normal_Curve.html">8.2. Standard Normal Curve</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/03_Normal_Approximation.html">8.3. Normal Approximation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/04_How_Large_is_Large.html">8.4. How Large is “Large”?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_08/05_Exercises.html">8.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_09/00_Inference.html">9. Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/01_Confidence_Intervals_Method.html">9.1. Confidence Intervals: Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/02_Confidence_Intervals_Interpretation.html">9.2. Confidence Intervals: Interpretation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/03_Testing_Hypotheses.html">9.3. Testing Hypotheses</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/04_AB_Testing_Fishers_Exact_Test.html">9.4. A/B Testing: Fisher’s Exact Test</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_09/05_Exercises.html">9.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_10/00_Probability_Density.html">10. Probability Density</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/01_Density.html">10.1. Density</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/02_Expectation_and_Variance.html">10.2. Expectation and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/03_The_Exponential_Distribution.html">10.3. Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/04_The_Normal_Distribution.html">10.4. The Normal Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_10/05_Exercises.html">10.5. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_11/00_Bias_Variance_and_Least_Squares.html">11. Bias, Variance, and Least Squares</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/01_Bias_and_Variance.html">11.1. Bias and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/02_Examples.html">11.2. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/03_Least_Squares_Linear_Regression.html">11.3. Least Squares Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/04_Bounds_on_Correlation.html">11.4. Bounds on Correlation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/05_The_Error_in_Regression.html">11.5. The Error in Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_11/06_Exercises.html">11.6. Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Chapter_12/00_Inference_in_Regression.html">12. Inference in Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/01_The_Simple_Linear_Regression_Model.html">12.1. The Simple Linear Regression Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/02_The_Distribution_of_the_Estimated_Slope.html">12.2. The Distribution of the Estimated Slope</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/03_Towards_Multiple_Regression.html">12.3. Towards Multiple Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Chapter_12/04_Exercises.html">12.4. Exercises</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/content/Chapter_05/04_Unbiased_Estimators.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Unbiased Estimators</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-linear-function-rule">5.4.1. Preliminary: Linear Function Rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology">5.4.2. Terminology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiased-estimator">5.4.3. Unbiased Estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-mean">5.4.4. Sample Mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-proportion">5.4.5. Sample Proportion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-largest-possible-value">5.4.6. Estimating the Largest Possible Value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#world-war-ii-tanks">5.4.7. World War II Tanks</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell tag_remove_input docutils container">
</div>
<section id="unbiased-estimators">
<span id="ch5-4"></span><h1><span class="section-number">5.4. </span>Unbiased Estimators<a class="headerlink" href="#unbiased-estimators" title="Link to this heading">#</a></h1>
<p>Data scientists often use information in random samples to estimate unknown numercial quantities. For example, they might estimate the unknown average income in a large population by using incomes in a random sample drawn from the population. In this section we will examine one criterion for a good estimate.</p>
<p>First let’s note a straightforward but extremely handy property of expectation.</p>
<section id="preliminary-linear-function-rule">
<h2><span class="section-number">5.4.1. </span>Preliminary: Linear Function Rule<a class="headerlink" href="#preliminary-linear-function-rule" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable and let <span class="math notranslate nohighlight">\(Y = aX + b\)</span>. Then <span class="math notranslate nohighlight">\(Y\)</span> is a linear function of <span class="math notranslate nohighlight">\(X\)</span>. By our method for finding the expectation of a function of a random variable,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
E(Y) ~ = ~ E(aX + b) ~ &amp;= ~ \sum_{\text{all }x} (ax+b)P(X=x) \\
&amp;= ~ a\sum_{\text{all }x} xP(X=x) ~ + ~ b \sum_{\text{all }x} P(X=x) \\
&amp;= ~ aE(X) + b
\end{align*}
\end{split}\]</div>
<p>Thus the <em>linear function rule</em> is that the expectation of a linear function of a random variable is equal to the linear function of the expectation.</p>
<p>This switching of functions and expectations is in general false for non-linear functions, as we saw earlier. But the fact that it is true for linear functions makes calculations easier.</p>
<p>For example, <span class="math notranslate nohighlight">\(E(10 - X) = 10 - E(X)\)</span>, <span class="math notranslate nohighlight">\(E((X-2)/3) = (E(X)-2)/3\)</span>, and so on.</p>
</section>
<section id="terminology">
<h2><span class="section-number">5.4.2. </span>Terminology<a class="headerlink" href="#terminology" title="Link to this heading">#</a></h2>
<p>In the context of estimation, a <em>parameter</em> is a fixed number associated with the population. That’s the same as the way we have used the term before: the parameter is a constant in the distribution of each sampled element.</p>
<p>For example, if the population consists of all U.S. adults, the parameter could be the average annual income in the population. We will denote this parameter by <span class="math notranslate nohighlight">\(\mu\)</span> for “mean”. Data scientists commonly use <span class="math notranslate nohighlight">\(\mu\)</span> to represent means, in vastly different contexts. When you read the description of a model or an analysis and see the notation <span class="math notranslate nohighlight">\(\mu\)</span>, make sure you understand exactly how it is defined in that context.</p>
<p>Now suppose you draw a random sample from the population. A <em>statistic</em> is any number computed based on the data in the sample. Thus for example the average income of the sampled people is a statistic.</p>
<p>In general, if <span class="math notranslate nohighlight">\(X_i\)</span> represents the <span class="math notranslate nohighlight">\(i\)</span>th element in the sample, then a statistic is a function <span class="math notranslate nohighlight">\(g(X_1, X_2, \ldots, X_n)\)</span>. The sample average is the statistic <span class="math notranslate nohighlight">\(\bar{X}\)</span> defined as the function</p>
<div class="math notranslate nohighlight">
\[
\bar{X} ~ = ~ \frac{1}{n} \sum_{i=1}^n X_i
\]</div>
<p>One important difference between a parameter and a statistic, as they have been defined above, is that a parameter is a fixed but possibly unknown number, whereas a statistic is a random variable. The value of the statistic depends on the elements that get randomly selected to be in the sample.</p>
<p>In our example about incomes, the parameter <span class="math notranslate nohighlight">\(\mu\)</span> is the average income in the whole population. Even if we don’t know what it is, it’s a fixed number. The statistic <span class="math notranslate nohighlight">\(\bar{X}\)</span> is the average income in the sample. This is a random quantity since it depends on <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> which are all random variables.</p>
<p>If a statistic is being used to estimate a parameter, the statistic is sometimes called an <em>estimator</em> of the parameter.</p>
<p>Thus if you use the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span> to estimate the population mean <span class="math notranslate nohighlight">\(\mu\)</span>, then <span class="math notranslate nohighlight">\(\bar{X}\)</span> is an estimator of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>This section is about a property that is often – but not always – considered desirable in an estimator.</p>
</section>
<section id="unbiased-estimator">
<h2><span class="section-number">5.4.3. </span>Unbiased Estimator<a class="headerlink" href="#unbiased-estimator" title="Link to this heading">#</a></h2>
<p>An unbiased estimator of a parameter is an estimator whose expected value is equal to the parameter.</p>
<p>That is, if the estimator <span class="math notranslate nohighlight">\(S\)</span> is being used to estimate a parameter <span class="math notranslate nohighlight">\(\theta\)</span>, then <span class="math notranslate nohighlight">\(S\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\theta\)</span> if <span class="math notranslate nohighlight">\(E(S) = \theta\)</span>.</p>
<p>Remember that expectation can be thought of as a long-run average value of a random variable. If an estimator <span class="math notranslate nohighlight">\(S\)</span> is unbiased, then on average it is equal to the number it is trying to estimate. Here “on average” involves imagining repeated samples, as follows:</p>
<ul class="simple">
<li><p>Draw one random sample; compute the value of <span class="math notranslate nohighlight">\(S\)</span> based on that sample.</p></li>
<li><p>Draw another random sample of the same size, independently of the first one; compute the value of <span class="math notranslate nohighlight">\(S\)</span> based on this sample.</p></li>
<li><p>Repeat the step above as many times as you can.</p></li>
<li><p>You will now have lots of observed values of <span class="math notranslate nohighlight">\(S\)</span>.</p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(S\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\theta\)</span>, then on average, these values will neither be greater than <span class="math notranslate nohighlight">\(\theta\)</span> nor smaller than <span class="math notranslate nohighlight">\(\theta\)</span>. On average in the long run they will be just right: equal to <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>This seems like a good property for an estimator to have. In many settings, natural estimators turn out to be unbiased. Let’s look at some examples.</p>
</section>
<section id="sample-mean">
<h2><span class="section-number">5.4.4. </span>Sample Mean<a class="headerlink" href="#sample-mean" title="Link to this heading">#</a></h2>
<p>Suppose you want to estimate the mean of a population based on a sample <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> drawn at random with replacement from the population.</p>
<p>It is natural to want to use the sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span> as an estimator of the population mean <span class="math notranslate nohighlight">\(\mu\)</span>. To see whether <span class="math notranslate nohighlight">\(\bar{X}\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span> we have to calculate its expectation. We can do this by using the linear function rule and additivity.</p>
<div class="math notranslate nohighlight">
\[
E(\bar{X}) ~ = ~ E\big{(} \frac{1}{n}\sum_{i=1}^n X_i \big{)} ~ = ~ \frac{1}{n}\sum_{i=1}^n E(X_i) ~ = ~ \frac{1}{n} \cdot n\mu ~ = ~ \mu
\]</div>
<p>Thus <span class="math notranslate nohighlight">\(\bar{X}\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>Notice that in the calculation above we have also discovered many other unbiased estimators of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>For example, <span class="math notranslate nohighlight">\(X_1\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span> because <span class="math notranslate nohighlight">\(E(X_1) = \mu\)</span>. Indeed if you fix any <span class="math notranslate nohighlight">\(i\)</span> then <span class="math notranslate nohighlight">\(X_i\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>Even though both <span class="math notranslate nohighlight">\(\bar{X}\)</span> and <span class="math notranslate nohighlight">\(X_1\)</span> are unbiased estimators, it seems like a better idea to use <span class="math notranslate nohighlight">\(\bar{X}\)</span> to estimate <span class="math notranslate nohighlight">\(\mu\)</span> than to use just <span class="math notranslate nohighlight">\(X_1\)</span>. Why throw away the rest of the data?</p>
<p>This intution is correct: it is indeed better to use <span class="math notranslate nohighlight">\(\bar{X}\)</span>, because it is likely to be closer to <span class="math notranslate nohighlight">\(\mu\)</span> than <span class="math notranslate nohighlight">\(X_1\)</span>. We will show this later in the course. For now, just note that the same sample can be used to construct more than one unbiased estimator for the parameter.</p>
</section>
<section id="sample-proportion">
<h2><span class="section-number">5.4.5. </span>Sample Proportion<a class="headerlink" href="#sample-proportion" title="Link to this heading">#</a></h2>
<p>An important special case of the sample mean is when the population consists of zeros and ones.</p>
<p>You know that the sum of a sequence of zeros and ones is equal to the number of ones in the sequence. It follows that the average of a sequence of zeros and ones is the proportion of ones in the sequence.</p>
<p>Suppose a population has a proportion <span class="math notranslate nohighlight">\(p\)</span> of ones and <span class="math notranslate nohighlight">\(1-p\)</span> of zeros. Then the mean of the population is <span class="math notranslate nohighlight">\(p\)</span>, the population proportion of ones.</p>
<p>Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> be draws at random with replacement from the population. Then <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> are independent identically distributed indicator random variables, each with chance <span class="math notranslate nohighlight">\(p\)</span> of being 1.</p>
<p>The sample mean <span class="math notranslate nohighlight">\(\bar{X}\)</span> is the <em>sample proportion</em> of ones, and is an unbiased estimator of the population proportion of ones.</p>
<p>Note that in this case the <em>sample sum</em> <span class="math notranslate nohighlight">\(S_n = X_1 + X_2 + \ldots + X_n\)</span> is the number of ones in the sample and has the binomial <span class="math notranslate nohighlight">\((n, p)\)</span> distribution. The sample mean is <span class="math notranslate nohighlight">\(\bar{X} = S_n/n\)</span>.</p>
<p>The graph below shows the relation between the sample proportion <span class="math notranslate nohighlight">\(\bar{X}\)</span> and the population proportion <span class="math notranslate nohighlight">\(p\)</span> in an example.</p>
<p>Suppose you roll a die 30 times and find the sample proportion of sixes. The histogram below shows the results of 20,000 repetitions of this experiment. On average, the 20,000 sample proportions are almost indistinguishable from <span class="math notranslate nohighlight">\(p = 1/6\)</span>.</p>
<div class="cell tag_remove_input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>n = 30
p = 0.1667
Average of observed sample proportions = 0.1666
</pre></div>
</div>
<img alt="../../_images/0ed79741a9f3edff6a46c787ac196bbc8bbb11ce4b1042be6759e9e7029b06fb.png" src="../../_images/0ed79741a9f3edff6a46c787ac196bbc8bbb11ce4b1042be6759e9e7029b06fb.png" />
</div>
</div>
</section>
<section id="estimating-the-largest-possible-value">
<h2><span class="section-number">5.4.6. </span>Estimating the Largest Possible Value<a class="headerlink" href="#estimating-the-largest-possible-value" title="Link to this heading">#</a></h2>
<p>Suppose <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_n\)</span> are independent and identically distributed (i.i.d.), each uniform on <span class="math notranslate nohighlight">\(1, 2, 3, \ldots, N\)</span> for some fixed but unknown <span class="math notranslate nohighlight">\(N\)</span>. Let us construct an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>The population mean is <span class="math notranslate nohighlight">\((N+1)/2\)</span>. If <span class="math notranslate nohighlight">\(\bar{X}\)</span> is the sample mean then</p>
<div class="math notranslate nohighlight">
\[
E(\bar{X}) ~ = ~ \frac{N+1}{2}
\]</div>
<p>so <span class="math notranslate nohighlight">\(\bar{X}\)</span> is not an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>. We wouldn’t expect it to be, because <span class="math notranslate nohighlight">\(N\)</span> is the largest any of the sampled elements could be whereas <span class="math notranslate nohighlight">\(\bar{X}\)</span> is likely to be somewhere in the middle of the sample.</p>
<p>But we can see that</p>
<div class="math notranslate nohighlight">
\[
2E(\bar{X}) - 1 ~ = ~ N
\]</div>
<p>By the linear function rule,</p>
<div class="math notranslate nohighlight">
\[
2E(\bar{X}) - 1 ~ = ~ E(2\bar{X} - 1)
\]</div>
<p>So the statistic <span class="math notranslate nohighlight">\(T = 2\bar{X} - 1\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p>
</section>
<section id="world-war-ii-tanks">
<span id="ch5-4-7"></span><h2><span class="section-number">5.4.7. </span>World War II Tanks<a class="headerlink" href="#world-war-ii-tanks" title="Link to this heading">#</a></h2>
<p>The calculation above stems from a problem the Allied forces faced in World War II. Germany had a seemingly never-ending fleet of Panzer tanks, and the Allies needed to estimate how many they had. They decided to base their estimates on the serial numbers of the tanks that they saw.</p>
<p>Here is a picture of one from <a class="reference external" href="https://en.wikipedia.org/wiki/Panzer_IV">Wikipedia</a>.</p>
<p><img alt="Panzer Tank" src="../../_images/panzer.png" /></p>
<p>Notice the serial number on the top left. When tanks were disabled or destroyed, it was discovered that their parts had serial numbers too. The ones from the gear boxes proved very useful.</p>
<p>The idea was to model the observed serial numbers as random draws from <span class="math notranslate nohighlight">\(1, 2, \ldots, N\)</span> and then estimate <span class="math notranslate nohighlight">\(N\)</span>. This is of course a very simplified model of reality, and we will make some additional simplifications. But estimates based on even such simple probabilistic models proved to be quite a bit <a class="reference external" href="https://en.wikipedia.org/wiki/German_tank_problem#Specific_data">more accurate</a> than those based on the intelligence gathered by the Allies. For example, in August 1942, intelligence estimates were that Germany was producing 1,550 tanks per month. The prediction based on the probability model was 327 per month. After the war, German records showed that the actual production rate was 342 per month.</p>
<p>The model was that the draws were made at random without replacement from the integers 1 through <span class="math notranslate nohighlight">\(N\)</span>. But for even more simplicity, let’s pretend that the draws were made with replacement. That is, if we saw the same tank twice then we would record it twice.</p>
<p>In the example above, we constructed the random variable <span class="math notranslate nohighlight">\(T\)</span> to be an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>The Allied statisticians instead started with <span class="math notranslate nohighlight">\(M\)</span>, the sample maximum:</p>
<div class="math notranslate nohighlight">
\[
M ~ = ~ \max\{X_1, X_2, \ldots, X_n\}
\]</div>
<p>The sample maximum <span class="math notranslate nohighlight">\(M\)</span> is not an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>, because we know that its value is always less than or equal to <span class="math notranslate nohighlight">\(N\)</span>. Its average value therefore will be somewhat less than <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>But how much less? The histograms below show a comparison of the two estimates in the case where <span class="math notranslate nohighlight">\(N=300\)</span> and the sample size is <span class="math notranslate nohighlight">\(n=30\)</span>, based on 5,000 repetitions of the sampling process. Of course the Allies didn’t know <span class="math notranslate nohighlight">\(N\)</span>. But simulating the sample for “pretend” values of <span class="math notranslate nohighlight">\(N\)</span> helps us understand how the estimators behave.</p>
<div class="cell tag_remove_input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>N = 300
n = 30
5000 repetitions
</pre></div>
</div>
<img alt="../../_images/d843a1534de91be1b236bf4a07f10d1f91b11711beb8c26f702ca30377d74547.png" src="../../_images/d843a1534de91be1b236bf4a07f10d1f91b11711beb8c26f702ca30377d74547.png" />
</div>
</div>
<p>The histograms show that both estimators have pros and cons.</p>
<p>In one sense, <span class="math notranslate nohighlight">\(T\)</span> looks like a better estimator than <span class="math notranslate nohighlight">\(M\)</span>.</p>
<ul class="simple">
<li><p>The gold histogram shows the simulated distribution of <span class="math notranslate nohighlight">\(T = 2\bar{X} - 1\)</span>. The histogram is centered at 300, which is <span class="math notranslate nohighlight">\(N\)</span>. That’s because <span class="math notranslate nohighlight">\(T\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p>The blue histogram shows the simulated distribution of <span class="math notranslate nohighlight">\(M = \max\{X_1, X_2, \ldots, X_n\}\)</span>. This estimate is biased: the entire histogram is at or to the left of <span class="math notranslate nohighlight">\(N = 300\)</span>.</p></li>
</ul>
<p>On the other hand:</p>
<ul class="simple">
<li><p>The distribution of <span class="math notranslate nohighlight">\(T\)</span> is quite wide. The Allies would only see one sample, not 5,000 as have been simulated here. The graph shows that their estimate could be quite a bit too high or quite a bit too low, even though the average value of all 5,000 estimates is just about right.</p></li>
<li><p>The distribution of <span class="math notranslate nohighlight">\(M\)</span> is much narrower. Even though <span class="math notranslate nohighlight">\(M\)</span> could be an under-estimate, it doesn’t look as though it underestimates by much.</p></li>
</ul>
<p>That is justification for using <span class="math notranslate nohighlight">\(M\)</span> in place of <span class="math notranslate nohighlight">\(T\)</span> even though <span class="math notranslate nohighlight">\(T\)</span> is unbiased and <span class="math notranslate nohighlight">\(M\)</span> is not. Unbiasedness is a good property, but so is low variability.</p>
<p>In many situations, such as this one, lower bias tends to go with higher variability and vice versa. Data scientists call this the <em>bias-variance tradeoff.</em> We will examine it more carefully later in the course.</p>
<p>The Allied statisticians did better than we have done here. They used sampling without replacement as their model. Their estimates were remarkably close to the actual production rates obtained from German records after the war.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/Chapter_05"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="03_Method_of_Indicators.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5.3. </span>Method of Indicators</p>
      </div>
    </a>
    <a class="right-next"
       href="05_Conditional_Expectation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5.5. </span>Conditional Expectation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminary-linear-function-rule">5.4.1. Preliminary: Linear Function Rule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology">5.4.2. Terminology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiased-estimator">5.4.3. Unbiased Estimator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-mean">5.4.4. Sample Mean</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-proportion">5.4.5. Sample Proportion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-largest-possible-value">5.4.6. Estimating the Largest Possible Value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#world-war-ii-tanks">5.4.7. World War II Tanks</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ani Adhikari
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>